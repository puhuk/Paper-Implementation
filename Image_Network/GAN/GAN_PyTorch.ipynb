{
  "nbformat": 4,
  "nbformat_minor": 0, 
  "metadata": {
    "colab": {
      "name": "GAN_PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOKI2sp6mikE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "0a2c8f28-3830-4a59-a64d-da3b60644ae9"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive/Colab Notebooks/GAN/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du6yAhJ_mm8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ptAicCbmpJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "c3876668-bd0e-46c4-f2d0-b6d1424d414e"
      },
      "source": [
        "img_shape = (1, 28, 28)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.BatchNorm1d(128, 0.8),\n",
        "            nn.Linear(128,256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256,512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512,1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity\n",
        "\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "os.makedirs(\"mnist\", exist_ok=True)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.Resize(28), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
        "        ),\n",
        "    ),\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5,0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5,0.999))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 27785348.60it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 433683.13it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 6925265.36it/s]                           \n",
            "8192it [00:00, 163088.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMIOhW57mpOl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bafc5ff7-a2d5-4989-f6d3-3c40b0d62896"
      },
      "source": [
        "for epoch in range(30):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 28*28))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # print(\n",
        "        #     \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "        #     % (epoch, 30, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "        # )\n",
        "\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        if batches_done % 400 == 0:\n",
        "            print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, 30, i, len(dataloader), d_loss.item(), g_loss.item())\n",
        "          )\n",
        "            save_image(gen_imgs.data[:25], \"%d.png\" % batches_done, nrow=5, normalize=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/30] [Batch 0/3750] [D loss: 0.486675] [G loss: 0.841587]\n",
            "[Epoch 0/30] [Batch 400/3750] [D loss: 0.402024] [G loss: 0.987464]\n",
            "[Epoch 0/30] [Batch 800/3750] [D loss: 0.259497] [G loss: 1.154071]\n",
            "[Epoch 0/30] [Batch 1200/3750] [D loss: 0.208354] [G loss: 1.459342]\n",
            "[Epoch 0/30] [Batch 1600/3750] [D loss: 0.376257] [G loss: 1.252902]\n",
            "[Epoch 0/30] [Batch 2000/3750] [D loss: 0.383213] [G loss: 2.242895]\n",
            "[Epoch 0/30] [Batch 2400/3750] [D loss: 0.181606] [G loss: 1.928439]\n",
            "[Epoch 0/30] [Batch 2800/3750] [D loss: 0.588807] [G loss: 0.427038]\n",
            "[Epoch 0/30] [Batch 3200/3750] [D loss: 0.283773] [G loss: 3.228184]\n",
            "[Epoch 0/30] [Batch 3600/3750] [D loss: 0.256974] [G loss: 2.021714]\n",
            "[Epoch 1/30] [Batch 250/3750] [D loss: 0.154027] [G loss: 3.085303]\n",
            "[Epoch 1/30] [Batch 650/3750] [D loss: 0.226162] [G loss: 1.263692]\n",
            "[Epoch 1/30] [Batch 1050/3750] [D loss: 0.378017] [G loss: 1.244017]\n",
            "[Epoch 1/30] [Batch 1450/3750] [D loss: 0.106211] [G loss: 2.754150]\n",
            "[Epoch 1/30] [Batch 1850/3750] [D loss: 0.171863] [G loss: 1.654108]\n",
            "[Epoch 1/30] [Batch 2250/3750] [D loss: 0.189103] [G loss: 2.476835]\n",
            "[Epoch 1/30] [Batch 2650/3750] [D loss: 0.120477] [G loss: 2.916718]\n",
            "[Epoch 1/30] [Batch 3050/3750] [D loss: 0.148521] [G loss: 2.660342]\n",
            "[Epoch 1/30] [Batch 3450/3750] [D loss: 0.126971] [G loss: 2.823060]\n",
            "[Epoch 2/30] [Batch 100/3750] [D loss: 0.114774] [G loss: 4.597096]\n",
            "[Epoch 2/30] [Batch 500/3750] [D loss: 0.089092] [G loss: 4.407806]\n",
            "[Epoch 2/30] [Batch 900/3750] [D loss: 0.120791] [G loss: 2.018706]\n",
            "[Epoch 2/30] [Batch 1300/3750] [D loss: 0.380424] [G loss: 6.541331]\n",
            "[Epoch 2/30] [Batch 1700/3750] [D loss: 0.368299] [G loss: 1.236752]\n",
            "[Epoch 2/30] [Batch 2100/3750] [D loss: 0.122216] [G loss: 1.780865]\n",
            "[Epoch 2/30] [Batch 2500/3750] [D loss: 0.166382] [G loss: 1.797073]\n",
            "[Epoch 2/30] [Batch 2900/3750] [D loss: 0.404951] [G loss: 3.248827]\n",
            "[Epoch 2/30] [Batch 3300/3750] [D loss: 0.140675] [G loss: 3.570508]\n",
            "[Epoch 2/30] [Batch 3700/3750] [D loss: 0.140665] [G loss: 3.085233]\n",
            "[Epoch 3/30] [Batch 350/3750] [D loss: 0.233743] [G loss: 2.385670]\n",
            "[Epoch 3/30] [Batch 750/3750] [D loss: 0.420432] [G loss: 2.084725]\n",
            "[Epoch 3/30] [Batch 1150/3750] [D loss: 0.384658] [G loss: 1.617189]\n",
            "[Epoch 3/30] [Batch 1550/3750] [D loss: 0.597796] [G loss: 2.435034]\n",
            "[Epoch 3/30] [Batch 1950/3750] [D loss: 0.272111] [G loss: 2.766519]\n",
            "[Epoch 3/30] [Batch 2350/3750] [D loss: 0.280434] [G loss: 1.933477]\n",
            "[Epoch 3/30] [Batch 2750/3750] [D loss: 0.110738] [G loss: 2.363568]\n",
            "[Epoch 3/30] [Batch 3150/3750] [D loss: 0.342411] [G loss: 4.182507]\n",
            "[Epoch 3/30] [Batch 3550/3750] [D loss: 0.571402] [G loss: 2.637020]\n",
            "[Epoch 4/30] [Batch 200/3750] [D loss: 0.200428] [G loss: 1.717353]\n",
            "[Epoch 4/30] [Batch 600/3750] [D loss: 0.353821] [G loss: 2.008726]\n",
            "[Epoch 4/30] [Batch 1000/3750] [D loss: 0.451845] [G loss: 1.832590]\n",
            "[Epoch 4/30] [Batch 1400/3750] [D loss: 0.314565] [G loss: 0.964004]\n",
            "[Epoch 4/30] [Batch 1800/3750] [D loss: 0.219262] [G loss: 1.677032]\n",
            "[Epoch 4/30] [Batch 2200/3750] [D loss: 0.326589] [G loss: 2.649599]\n",
            "[Epoch 4/30] [Batch 2600/3750] [D loss: 0.582489] [G loss: 0.655714]\n",
            "[Epoch 4/30] [Batch 3000/3750] [D loss: 0.435940] [G loss: 2.610555]\n",
            "[Epoch 4/30] [Batch 3400/3750] [D loss: 0.533779] [G loss: 0.870055]\n",
            "[Epoch 5/30] [Batch 50/3750] [D loss: 0.338065] [G loss: 1.707911]\n",
            "[Epoch 5/30] [Batch 450/3750] [D loss: 0.475697] [G loss: 1.064377]\n",
            "[Epoch 5/30] [Batch 850/3750] [D loss: 0.463754] [G loss: 0.927422]\n",
            "[Epoch 5/30] [Batch 1250/3750] [D loss: 0.529418] [G loss: 1.650679]\n",
            "[Epoch 5/30] [Batch 1650/3750] [D loss: 0.418840] [G loss: 1.377346]\n",
            "[Epoch 5/30] [Batch 2050/3750] [D loss: 0.645580] [G loss: 2.179577]\n",
            "[Epoch 5/30] [Batch 2450/3750] [D loss: 0.560576] [G loss: 0.739002]\n",
            "[Epoch 5/30] [Batch 2850/3750] [D loss: 0.492642] [G loss: 0.742543]\n",
            "[Epoch 5/30] [Batch 3250/3750] [D loss: 0.409336] [G loss: 1.398352]\n",
            "[Epoch 5/30] [Batch 3650/3750] [D loss: 0.538560] [G loss: 0.750541]\n",
            "[Epoch 6/30] [Batch 300/3750] [D loss: 0.492921] [G loss: 1.489961]\n",
            "[Epoch 6/30] [Batch 700/3750] [D loss: 0.474223] [G loss: 1.510862]\n",
            "[Epoch 6/30] [Batch 1100/3750] [D loss: 0.388994] [G loss: 1.426785]\n",
            "[Epoch 6/30] [Batch 1500/3750] [D loss: 0.400621] [G loss: 1.907855]\n",
            "[Epoch 6/30] [Batch 1900/3750] [D loss: 0.502954] [G loss: 1.583751]\n",
            "[Epoch 6/30] [Batch 2300/3750] [D loss: 0.422139] [G loss: 1.043823]\n",
            "[Epoch 6/30] [Batch 2700/3750] [D loss: 0.453036] [G loss: 2.477822]\n",
            "[Epoch 6/30] [Batch 3100/3750] [D loss: 0.522130] [G loss: 1.246411]\n",
            "[Epoch 6/30] [Batch 3500/3750] [D loss: 0.418505] [G loss: 1.547432]\n",
            "[Epoch 7/30] [Batch 150/3750] [D loss: 0.589073] [G loss: 0.932674]\n",
            "[Epoch 7/30] [Batch 550/3750] [D loss: 0.555128] [G loss: 2.158962]\n",
            "[Epoch 7/30] [Batch 950/3750] [D loss: 0.540438] [G loss: 1.385791]\n",
            "[Epoch 7/30] [Batch 1350/3750] [D loss: 0.518611] [G loss: 1.559430]\n",
            "[Epoch 7/30] [Batch 1750/3750] [D loss: 0.391263] [G loss: 1.353977]\n",
            "[Epoch 7/30] [Batch 2150/3750] [D loss: 0.466006] [G loss: 1.149483]\n",
            "[Epoch 7/30] [Batch 2550/3750] [D loss: 0.537799] [G loss: 2.076648]\n",
            "[Epoch 7/30] [Batch 2950/3750] [D loss: 0.323171] [G loss: 1.466694]\n",
            "[Epoch 7/30] [Batch 3350/3750] [D loss: 0.499385] [G loss: 1.530328]\n",
            "[Epoch 8/30] [Batch 0/3750] [D loss: 0.470742] [G loss: 1.080245]\n",
            "[Epoch 8/30] [Batch 400/3750] [D loss: 0.388444] [G loss: 1.487909]\n",
            "[Epoch 8/30] [Batch 800/3750] [D loss: 0.451697] [G loss: 1.130653]\n",
            "[Epoch 8/30] [Batch 1200/3750] [D loss: 0.608897] [G loss: 0.845689]\n",
            "[Epoch 8/30] [Batch 1600/3750] [D loss: 0.558198] [G loss: 1.730814]\n",
            "[Epoch 8/30] [Batch 2000/3750] [D loss: 0.692523] [G loss: 0.903523]\n",
            "[Epoch 8/30] [Batch 2400/3750] [D loss: 0.574602] [G loss: 1.454741]\n",
            "[Epoch 8/30] [Batch 2800/3750] [D loss: 0.494937] [G loss: 1.529370]\n",
            "[Epoch 8/30] [Batch 3200/3750] [D loss: 0.507228] [G loss: 1.793832]\n",
            "[Epoch 8/30] [Batch 3600/3750] [D loss: 0.465610] [G loss: 1.527907]\n",
            "[Epoch 9/30] [Batch 250/3750] [D loss: 0.722544] [G loss: 2.248681]\n",
            "[Epoch 9/30] [Batch 650/3750] [D loss: 0.390037] [G loss: 0.907394]\n",
            "[Epoch 9/30] [Batch 1050/3750] [D loss: 0.607881] [G loss: 1.204733]\n",
            "[Epoch 9/30] [Batch 1450/3750] [D loss: 0.390023] [G loss: 0.856631]\n",
            "[Epoch 9/30] [Batch 1850/3750] [D loss: 0.598484] [G loss: 1.231691]\n",
            "[Epoch 9/30] [Batch 2250/3750] [D loss: 0.498908] [G loss: 0.865795]\n",
            "[Epoch 9/30] [Batch 2650/3750] [D loss: 0.462472] [G loss: 0.933068]\n",
            "[Epoch 9/30] [Batch 3050/3750] [D loss: 0.376003] [G loss: 1.060539]\n",
            "[Epoch 9/30] [Batch 3450/3750] [D loss: 0.649056] [G loss: 1.746746]\n",
            "[Epoch 10/30] [Batch 100/3750] [D loss: 0.464839] [G loss: 1.238153]\n",
            "[Epoch 10/30] [Batch 500/3750] [D loss: 0.425114] [G loss: 1.272608]\n",
            "[Epoch 10/30] [Batch 900/3750] [D loss: 0.635041] [G loss: 1.017488]\n",
            "[Epoch 10/30] [Batch 1300/3750] [D loss: 0.453039] [G loss: 0.952563]\n",
            "[Epoch 10/30] [Batch 1700/3750] [D loss: 0.681434] [G loss: 1.057626]\n",
            "[Epoch 10/30] [Batch 2100/3750] [D loss: 0.509416] [G loss: 1.244830]\n",
            "[Epoch 10/30] [Batch 2500/3750] [D loss: 0.510698] [G loss: 0.925606]\n",
            "[Epoch 10/30] [Batch 2900/3750] [D loss: 0.576815] [G loss: 0.650209]\n",
            "[Epoch 10/30] [Batch 3300/3750] [D loss: 0.460160] [G loss: 1.081073]\n",
            "[Epoch 10/30] [Batch 3700/3750] [D loss: 0.474060] [G loss: 1.460545]\n",
            "[Epoch 11/30] [Batch 350/3750] [D loss: 0.489548] [G loss: 0.749001]\n",
            "[Epoch 11/30] [Batch 750/3750] [D loss: 0.651296] [G loss: 2.038675]\n",
            "[Epoch 11/30] [Batch 1150/3750] [D loss: 0.509936] [G loss: 1.034822]\n",
            "[Epoch 11/30] [Batch 1550/3750] [D loss: 0.360969] [G loss: 1.875956]\n",
            "[Epoch 11/30] [Batch 1950/3750] [D loss: 0.481786] [G loss: 1.974747]\n",
            "[Epoch 11/30] [Batch 2350/3750] [D loss: 0.337380] [G loss: 1.094107]\n",
            "[Epoch 11/30] [Batch 2750/3750] [D loss: 0.563509] [G loss: 1.751859]\n",
            "[Epoch 11/30] [Batch 3150/3750] [D loss: 0.511147] [G loss: 1.672292]\n",
            "[Epoch 11/30] [Batch 3550/3750] [D loss: 0.601927] [G loss: 1.398723]\n",
            "[Epoch 12/30] [Batch 200/3750] [D loss: 0.466616] [G loss: 1.014249]\n",
            "[Epoch 12/30] [Batch 600/3750] [D loss: 0.729926] [G loss: 1.969269]\n",
            "[Epoch 12/30] [Batch 1000/3750] [D loss: 0.630783] [G loss: 0.571318]\n",
            "[Epoch 12/30] [Batch 1400/3750] [D loss: 0.583345] [G loss: 1.044517]\n",
            "[Epoch 12/30] [Batch 1800/3750] [D loss: 0.418211] [G loss: 1.129897]\n",
            "[Epoch 12/30] [Batch 2200/3750] [D loss: 0.568845] [G loss: 0.951473]\n",
            "[Epoch 12/30] [Batch 2600/3750] [D loss: 0.477925] [G loss: 1.467721]\n",
            "[Epoch 12/30] [Batch 3000/3750] [D loss: 0.607795] [G loss: 1.728937]\n",
            "[Epoch 12/30] [Batch 3400/3750] [D loss: 0.424681] [G loss: 1.190181]\n",
            "[Epoch 13/30] [Batch 50/3750] [D loss: 0.509256] [G loss: 1.502461]\n",
            "[Epoch 13/30] [Batch 450/3750] [D loss: 0.621096] [G loss: 1.322760]\n",
            "[Epoch 13/30] [Batch 850/3750] [D loss: 0.524771] [G loss: 1.437392]\n",
            "[Epoch 13/30] [Batch 1250/3750] [D loss: 0.745151] [G loss: 2.582731]\n",
            "[Epoch 13/30] [Batch 1650/3750] [D loss: 0.505707] [G loss: 0.765915]\n",
            "[Epoch 13/30] [Batch 2050/3750] [D loss: 0.500474] [G loss: 1.383702]\n",
            "[Epoch 13/30] [Batch 2450/3750] [D loss: 0.509725] [G loss: 1.000046]\n",
            "[Epoch 13/30] [Batch 2850/3750] [D loss: 0.584498] [G loss: 1.325020]\n",
            "[Epoch 13/30] [Batch 3250/3750] [D loss: 0.510183] [G loss: 1.753633]\n",
            "[Epoch 13/30] [Batch 3650/3750] [D loss: 0.440069] [G loss: 0.821394]\n",
            "[Epoch 14/30] [Batch 300/3750] [D loss: 0.431463] [G loss: 0.781562]\n",
            "[Epoch 14/30] [Batch 700/3750] [D loss: 0.433417] [G loss: 1.252363]\n",
            "[Epoch 14/30] [Batch 1100/3750] [D loss: 0.426476] [G loss: 1.473484]\n",
            "[Epoch 14/30] [Batch 1500/3750] [D loss: 0.493042] [G loss: 1.539553]\n",
            "[Epoch 14/30] [Batch 1900/3750] [D loss: 0.441461] [G loss: 1.693664]\n",
            "[Epoch 14/30] [Batch 2300/3750] [D loss: 0.432642] [G loss: 0.997956]\n",
            "[Epoch 14/30] [Batch 2700/3750] [D loss: 0.511154] [G loss: 1.250938]\n",
            "[Epoch 14/30] [Batch 3100/3750] [D loss: 0.467039] [G loss: 0.974037]\n",
            "[Epoch 14/30] [Batch 3500/3750] [D loss: 0.448775] [G loss: 1.044613]\n",
            "[Epoch 15/30] [Batch 150/3750] [D loss: 0.574137] [G loss: 1.942577]\n",
            "[Epoch 15/30] [Batch 550/3750] [D loss: 0.541608] [G loss: 0.973016]\n",
            "[Epoch 15/30] [Batch 950/3750] [D loss: 0.489141] [G loss: 1.102519]\n",
            "[Epoch 15/30] [Batch 1350/3750] [D loss: 0.434792] [G loss: 1.112460]\n",
            "[Epoch 15/30] [Batch 1750/3750] [D loss: 0.406493] [G loss: 1.580562]\n",
            "[Epoch 15/30] [Batch 2150/3750] [D loss: 0.428147] [G loss: 1.866148]\n",
            "[Epoch 15/30] [Batch 2550/3750] [D loss: 0.555039] [G loss: 0.956227]\n",
            "[Epoch 15/30] [Batch 2950/3750] [D loss: 0.501052] [G loss: 0.752250]\n",
            "[Epoch 15/30] [Batch 3350/3750] [D loss: 0.441409] [G loss: 1.167421]\n",
            "[Epoch 16/30] [Batch 0/3750] [D loss: 0.607203] [G loss: 1.707160]\n",
            "[Epoch 16/30] [Batch 400/3750] [D loss: 0.346088] [G loss: 0.982868]\n",
            "[Epoch 16/30] [Batch 800/3750] [D loss: 0.458916] [G loss: 0.879808]\n",
            "[Epoch 16/30] [Batch 1200/3750] [D loss: 0.442209] [G loss: 1.925268]\n",
            "[Epoch 16/30] [Batch 1600/3750] [D loss: 0.571829] [G loss: 1.040049]\n",
            "[Epoch 16/30] [Batch 2000/3750] [D loss: 0.555891] [G loss: 1.453578]\n",
            "[Epoch 16/30] [Batch 2400/3750] [D loss: 0.568596] [G loss: 1.630254]\n",
            "[Epoch 16/30] [Batch 2800/3750] [D loss: 0.488045] [G loss: 0.956266]\n",
            "[Epoch 16/30] [Batch 3200/3750] [D loss: 0.462047] [G loss: 0.996794]\n",
            "[Epoch 16/30] [Batch 3600/3750] [D loss: 0.657804] [G loss: 1.605993]\n",
            "[Epoch 17/30] [Batch 250/3750] [D loss: 0.605358] [G loss: 0.910856]\n",
            "[Epoch 17/30] [Batch 650/3750] [D loss: 0.503198] [G loss: 1.314627]\n",
            "[Epoch 17/30] [Batch 1050/3750] [D loss: 0.516151] [G loss: 1.817107]\n",
            "[Epoch 17/30] [Batch 1450/3750] [D loss: 0.515079] [G loss: 1.472226]\n",
            "[Epoch 17/30] [Batch 1850/3750] [D loss: 0.335382] [G loss: 1.512251]\n",
            "[Epoch 17/30] [Batch 2250/3750] [D loss: 0.386083] [G loss: 1.400486]\n",
            "[Epoch 17/30] [Batch 2650/3750] [D loss: 0.465061] [G loss: 1.616205]\n",
            "[Epoch 17/30] [Batch 3050/3750] [D loss: 0.476789] [G loss: 1.349353]\n",
            "[Epoch 17/30] [Batch 3450/3750] [D loss: 0.522564] [G loss: 1.226047]\n",
            "[Epoch 18/30] [Batch 100/3750] [D loss: 0.326867] [G loss: 1.289952]\n",
            "[Epoch 18/30] [Batch 500/3750] [D loss: 0.362858] [G loss: 1.267998]\n",
            "[Epoch 18/30] [Batch 900/3750] [D loss: 0.468721] [G loss: 0.834168]\n",
            "[Epoch 18/30] [Batch 1300/3750] [D loss: 0.422225] [G loss: 1.527343]\n",
            "[Epoch 18/30] [Batch 1700/3750] [D loss: 0.527873] [G loss: 0.815560]\n",
            "[Epoch 18/30] [Batch 2100/3750] [D loss: 0.331337] [G loss: 1.444729]\n",
            "[Epoch 18/30] [Batch 2500/3750] [D loss: 0.395727] [G loss: 1.483425]\n",
            "[Epoch 18/30] [Batch 2900/3750] [D loss: 0.447290] [G loss: 2.055605]\n",
            "[Epoch 18/30] [Batch 3300/3750] [D loss: 0.536032] [G loss: 1.421622]\n",
            "[Epoch 18/30] [Batch 3700/3750] [D loss: 0.393748] [G loss: 1.520177]\n",
            "[Epoch 19/30] [Batch 350/3750] [D loss: 0.437872] [G loss: 1.149822]\n",
            "[Epoch 19/30] [Batch 750/3750] [D loss: 0.405722] [G loss: 1.341162]\n",
            "[Epoch 19/30] [Batch 1150/3750] [D loss: 0.383679] [G loss: 1.392463]\n",
            "[Epoch 19/30] [Batch 1550/3750] [D loss: 0.439811] [G loss: 1.441024]\n",
            "[Epoch 19/30] [Batch 1950/3750] [D loss: 0.464180] [G loss: 0.941823]\n",
            "[Epoch 19/30] [Batch 2350/3750] [D loss: 0.429695] [G loss: 1.701351]\n",
            "[Epoch 19/30] [Batch 2750/3750] [D loss: 0.408422] [G loss: 1.169847]\n",
            "[Epoch 19/30] [Batch 3150/3750] [D loss: 0.501105] [G loss: 0.909852]\n",
            "[Epoch 19/30] [Batch 3550/3750] [D loss: 0.533409] [G loss: 2.185749]\n",
            "[Epoch 20/30] [Batch 200/3750] [D loss: 0.474706] [G loss: 1.333587]\n",
            "[Epoch 20/30] [Batch 600/3750] [D loss: 0.518482] [G loss: 1.409488]\n",
            "[Epoch 20/30] [Batch 1000/3750] [D loss: 0.576324] [G loss: 1.180444]\n",
            "[Epoch 20/30] [Batch 1400/3750] [D loss: 0.418277] [G loss: 1.611111]\n",
            "[Epoch 20/30] [Batch 1800/3750] [D loss: 0.529217] [G loss: 2.243502]\n",
            "[Epoch 20/30] [Batch 2200/3750] [D loss: 0.441987] [G loss: 1.590940]\n",
            "[Epoch 20/30] [Batch 2600/3750] [D loss: 0.530480] [G loss: 1.127217]\n",
            "[Epoch 20/30] [Batch 3000/3750] [D loss: 0.550009] [G loss: 0.789259]\n",
            "[Epoch 20/30] [Batch 3400/3750] [D loss: 0.653005] [G loss: 0.505539]\n",
            "[Epoch 21/30] [Batch 50/3750] [D loss: 0.482001] [G loss: 0.952445]\n",
            "[Epoch 21/30] [Batch 450/3750] [D loss: 0.538283] [G loss: 1.083122]\n",
            "[Epoch 21/30] [Batch 850/3750] [D loss: 0.563444] [G loss: 0.936011]\n",
            "[Epoch 21/30] [Batch 1250/3750] [D loss: 0.441429] [G loss: 1.248977]\n",
            "[Epoch 21/30] [Batch 1650/3750] [D loss: 0.481213] [G loss: 1.522187]\n",
            "[Epoch 21/30] [Batch 2050/3750] [D loss: 0.458951] [G loss: 0.985221]\n",
            "[Epoch 21/30] [Batch 2450/3750] [D loss: 0.391460] [G loss: 1.309056]\n",
            "[Epoch 21/30] [Batch 2850/3750] [D loss: 0.570229] [G loss: 1.023628]\n",
            "[Epoch 21/30] [Batch 3250/3750] [D loss: 0.363359] [G loss: 1.545969]\n",
            "[Epoch 21/30] [Batch 3650/3750] [D loss: 0.371372] [G loss: 1.681906]\n",
            "[Epoch 22/30] [Batch 300/3750] [D loss: 0.372164] [G loss: 1.422180]\n",
            "[Epoch 22/30] [Batch 700/3750] [D loss: 0.420417] [G loss: 1.083940]\n",
            "[Epoch 22/30] [Batch 1100/3750] [D loss: 0.505214] [G loss: 2.177252]\n",
            "[Epoch 22/30] [Batch 1500/3750] [D loss: 0.615815] [G loss: 0.654905]\n",
            "[Epoch 22/30] [Batch 1900/3750] [D loss: 0.482552] [G loss: 0.953255]\n",
            "[Epoch 22/30] [Batch 2300/3750] [D loss: 0.450291] [G loss: 1.367175]\n",
            "[Epoch 22/30] [Batch 2700/3750] [D loss: 0.585004] [G loss: 1.939714]\n",
            "[Epoch 22/30] [Batch 3100/3750] [D loss: 0.547406] [G loss: 1.379105]\n",
            "[Epoch 22/30] [Batch 3500/3750] [D loss: 0.497288] [G loss: 1.216098]\n",
            "[Epoch 23/30] [Batch 150/3750] [D loss: 0.479422] [G loss: 2.370384]\n",
            "[Epoch 23/30] [Batch 550/3750] [D loss: 0.437042] [G loss: 1.081493]\n",
            "[Epoch 23/30] [Batch 950/3750] [D loss: 0.512211] [G loss: 1.033046]\n",
            "[Epoch 23/30] [Batch 1350/3750] [D loss: 0.519700] [G loss: 1.457069]\n",
            "[Epoch 23/30] [Batch 1750/3750] [D loss: 0.506084] [G loss: 1.473080]\n",
            "[Epoch 23/30] [Batch 2150/3750] [D loss: 0.511656] [G loss: 1.469851]\n",
            "[Epoch 23/30] [Batch 2550/3750] [D loss: 0.322298] [G loss: 1.645260]\n",
            "[Epoch 23/30] [Batch 2950/3750] [D loss: 0.389086] [G loss: 1.290221]\n",
            "[Epoch 23/30] [Batch 3350/3750] [D loss: 0.473472] [G loss: 1.022892]\n",
            "[Epoch 24/30] [Batch 0/3750] [D loss: 0.548563] [G loss: 1.521193]\n",
            "[Epoch 24/30] [Batch 400/3750] [D loss: 0.439977] [G loss: 1.460950]\n",
            "[Epoch 24/30] [Batch 800/3750] [D loss: 0.427148] [G loss: 1.262892]\n",
            "[Epoch 24/30] [Batch 1200/3750] [D loss: 0.382870] [G loss: 1.451441]\n",
            "[Epoch 24/30] [Batch 1600/3750] [D loss: 0.561070] [G loss: 1.230191]\n",
            "[Epoch 24/30] [Batch 2000/3750] [D loss: 0.325865] [G loss: 1.586535]\n",
            "[Epoch 24/30] [Batch 2400/3750] [D loss: 0.395610] [G loss: 1.612185]\n",
            "[Epoch 24/30] [Batch 2800/3750] [D loss: 0.842064] [G loss: 0.713897]\n",
            "[Epoch 24/30] [Batch 3200/3750] [D loss: 0.418294] [G loss: 1.922484]\n",
            "[Epoch 24/30] [Batch 3600/3750] [D loss: 0.414456] [G loss: 1.144809]\n",
            "[Epoch 25/30] [Batch 250/3750] [D loss: 0.504512] [G loss: 0.779864]\n",
            "[Epoch 25/30] [Batch 650/3750] [D loss: 0.465156] [G loss: 1.196656]\n",
            "[Epoch 25/30] [Batch 1050/3750] [D loss: 0.450211] [G loss: 1.157244]\n",
            "[Epoch 25/30] [Batch 1450/3750] [D loss: 0.623838] [G loss: 0.763735]\n",
            "[Epoch 25/30] [Batch 1850/3750] [D loss: 0.488356] [G loss: 2.233095]\n",
            "[Epoch 25/30] [Batch 2250/3750] [D loss: 0.376564] [G loss: 1.480152]\n",
            "[Epoch 25/30] [Batch 2650/3750] [D loss: 0.460445] [G loss: 1.266639]\n",
            "[Epoch 25/30] [Batch 3050/3750] [D loss: 0.333559] [G loss: 1.842491]\n",
            "[Epoch 25/30] [Batch 3450/3750] [D loss: 0.394377] [G loss: 1.279491]\n",
            "[Epoch 26/30] [Batch 100/3750] [D loss: 0.457832] [G loss: 1.111537]\n",
            "[Epoch 26/30] [Batch 500/3750] [D loss: 0.442799] [G loss: 1.193446]\n",
            "[Epoch 26/30] [Batch 900/3750] [D loss: 0.485976] [G loss: 1.187915]\n",
            "[Epoch 26/30] [Batch 1300/3750] [D loss: 0.384699] [G loss: 1.719707]\n",
            "[Epoch 26/30] [Batch 1700/3750] [D loss: 0.338962] [G loss: 1.188146]\n",
            "[Epoch 26/30] [Batch 2100/3750] [D loss: 0.416929] [G loss: 1.518064]\n",
            "[Epoch 26/30] [Batch 2500/3750] [D loss: 0.535395] [G loss: 0.817306]\n",
            "[Epoch 26/30] [Batch 2900/3750] [D loss: 0.452834] [G loss: 1.344260]\n",
            "[Epoch 26/30] [Batch 3300/3750] [D loss: 0.553484] [G loss: 0.909261]\n",
            "[Epoch 26/30] [Batch 3700/3750] [D loss: 0.601723] [G loss: 1.195578]\n",
            "[Epoch 27/30] [Batch 350/3750] [D loss: 0.403907] [G loss: 1.187118]\n",
            "[Epoch 27/30] [Batch 750/3750] [D loss: 0.387776] [G loss: 1.364066]\n",
            "[Epoch 27/30] [Batch 1150/3750] [D loss: 0.484403] [G loss: 0.966498]\n",
            "[Epoch 27/30] [Batch 1550/3750] [D loss: 0.352327] [G loss: 1.053318]\n",
            "[Epoch 27/30] [Batch 1950/3750] [D loss: 0.431275] [G loss: 2.019079]\n",
            "[Epoch 27/30] [Batch 2350/3750] [D loss: 0.488201] [G loss: 1.219988]\n",
            "[Epoch 27/30] [Batch 2750/3750] [D loss: 0.387805] [G loss: 1.236731]\n",
            "[Epoch 27/30] [Batch 3150/3750] [D loss: 0.574331] [G loss: 1.275162]\n",
            "[Epoch 27/30] [Batch 3550/3750] [D loss: 0.504392] [G loss: 1.098174]\n",
            "[Epoch 28/30] [Batch 200/3750] [D loss: 0.506123] [G loss: 1.149142]\n",
            "[Epoch 28/30] [Batch 600/3750] [D loss: 0.355663] [G loss: 1.469326]\n",
            "[Epoch 28/30] [Batch 1000/3750] [D loss: 0.612089] [G loss: 1.302797]\n",
            "[Epoch 28/30] [Batch 1400/3750] [D loss: 0.413676] [G loss: 1.037617]\n",
            "[Epoch 28/30] [Batch 1800/3750] [D loss: 0.521523] [G loss: 1.504058]\n",
            "[Epoch 28/30] [Batch 2200/3750] [D loss: 0.492427] [G loss: 1.259524]\n",
            "[Epoch 28/30] [Batch 2600/3750] [D loss: 0.445938] [G loss: 1.112323]\n",
            "[Epoch 28/30] [Batch 3000/3750] [D loss: 0.617748] [G loss: 1.322044]\n",
            "[Epoch 28/30] [Batch 3400/3750] [D loss: 0.632378] [G loss: 1.447349]\n",
            "[Epoch 29/30] [Batch 50/3750] [D loss: 0.429285] [G loss: 1.096075]\n",
            "[Epoch 29/30] [Batch 450/3750] [D loss: 0.550153] [G loss: 1.457732]\n",
            "[Epoch 29/30] [Batch 850/3750] [D loss: 0.390296] [G loss: 1.306987]\n",
            "[Epoch 29/30] [Batch 1250/3750] [D loss: 0.402666] [G loss: 1.461366]\n",
            "[Epoch 29/30] [Batch 1650/3750] [D loss: 0.377677] [G loss: 1.257970]\n",
            "[Epoch 29/30] [Batch 2050/3750] [D loss: 0.353422] [G loss: 1.147838]\n",
            "[Epoch 29/30] [Batch 2450/3750] [D loss: 0.451499] [G loss: 1.096128]\n",
            "[Epoch 29/30] [Batch 2850/3750] [D loss: 0.474160] [G loss: 1.104410]\n",
            "[Epoch 29/30] [Batch 3250/3750] [D loss: 0.380849] [G loss: 1.063879]\n",
            "[Epoch 29/30] [Batch 3650/3750] [D loss: 0.494675] [G loss: 1.098731]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05eivQl-mm-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2WxwgfwmnBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxWlyfOcmnD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX0-eKogmnGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
