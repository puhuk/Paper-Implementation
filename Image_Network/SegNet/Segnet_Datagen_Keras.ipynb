{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2D, Reshape, Conv2DTranspose\n",
    "from keras.layers import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "import cv2\n",
    "\n",
    "from tensorflow.python.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img=[]\n",
    "train_mask=[]\n",
    "valid_img=[]\n",
    "valid_mask=[]\n",
    "\n",
    "train = open(\"../VOC_2011_TrainVal/VOCdevkit/VOC2011/ImageSets/Segmentation/train_2.txt\", 'r')\n",
    "lines = train.readlines()\n",
    "for i, line in enumerate(lines):\n",
    "    train_img.append(\"../VOC_2011_TrainVal/VOCdevkit/VOC2011/JPEGImages/\"+line.split('\\n')[0]+\".jpg\")\n",
    "    train_mask.append(\"../VOC_2011_TrainVal/VOCdevkit/VOC2011/SegmentationClass/\"+line.split('\\n')[0]+\".png\")\n",
    "train.close()\n",
    "\n",
    "valid = open(\"../VOC_2011_TrainVal/VOCdevkit/VOC2011/ImageSets/Segmentation/val_2.txt\", 'r')\n",
    "lines = valid.readlines()\n",
    "for i, line in enumerate(lines):\n",
    "    valid_img.append(\"../VOC_2011_TrainVal/VOCdevkit/VOC2011/JPEGImages/\"+line.split('\\n')[0]+\".jpg\")\n",
    "    valid_mask.append(\"../VOC_2011_TrainVal/VOCdevkit/VOC2011/SegmentationClass/\"+line.split('\\n')[0]+\".png\")\n",
    "valid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_label(labels, dims, n_labels):\n",
    "    x=np.zeros([dims[0], dims[1], n_labels])\n",
    "    for i in range(dims[0]):\n",
    "        for j in range(dims[1]):\n",
    "            x[i, j, labels[i][j]==1]    \n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def data_gen_small(img_list, mask_list, batch_size, dims, n_labels):\n",
    "    while True:\n",
    "        #ix = np.random.choice(np.arange(len(lists)), batch_size)\n",
    "        imgs = []\n",
    "        labels =[]\n",
    "        \n",
    "        for img_path in img_list:\n",
    "            original_img = cv2.imread(img_path)[:,:,::-1]\n",
    "            resized_img = cv2.resize(original_img, dims)\n",
    "            array_img = img_to_array(resized_img)/255.0\n",
    "            imgs.append(array_img)\n",
    "            \n",
    "        for mask_path in mask_list:\n",
    "            original_mask = cv2.imread(mask_path)\n",
    "            resized_mask = cv2.resize(original_mask, dims)\n",
    "            array_mask = category_label(resized_mask[:,:,0], dims, n_labels)\n",
    "            labels.append(array_mask)\n",
    "            \n",
    "        imgs = np.array(imgs)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        yield imgs, labels\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object data_gen_small at 0x0000022EF2202648>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rgb_to_onehot(rgb_image, colormap = VOC_COLORMAP):\n",
    "    '''Function to one hot encode RGB mask labels\n",
    "        Inputs: \n",
    "            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n",
    "    '''\n",
    "    num_classes = len(colormap)\n",
    "    shape = rgb_image.shape[:2]+(num_classes,)\n",
    "    encoded_image = np.zeros( shape, dtype=np.int8 )\n",
    "    for i, cls in enumerate(colormap):\n",
    "        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n",
    "    return encoded_image\n",
    "\n",
    "\n",
    "def onehot_to_rgb(onehot, colormap = VOC_COLORMAP):\n",
    "    '''Function to decode encoded mask labels\n",
    "        Inputs: \n",
    "            onehot - one hot encoded image matrix (height x width x num_classes)\n",
    "            colormap - dictionary of color to label id\n",
    "        Output: Decoded RGB image (height x width x 3) \n",
    "    '''\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "    for k in range(len(colormap)):\n",
    "        output[single_layer==k] = colormap[k]\n",
    "    return np.uint8(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting and resizing images ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e67e9541eb40879326f5c12d57e1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=124), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get and resize train images and masks\n",
    "def get_data(img_path_list, mask_path_list, train=True):\n",
    "    n_classes =21\n",
    "    X = np.zeros((len(img_path_list), 128, 128, 3), dtype=np.float32)\n",
    "    y = np.zeros((len(img_path_list), 128, 128, n_classes), dtype=np.float32)\n",
    "    \n",
    "    print('Getting and resizing images ... ')\n",
    "    for n, img in tqdm_notebook(enumerate(img_path_list), total=len(img_path_list)):\n",
    "        # Load images\n",
    "        img = load_img(img)\n",
    "        x_img = img_to_array(img)\n",
    "        x_img = resize(x_img, (128, 128), mode='constant', preserve_range=True)\n",
    "\n",
    "        mask = cv2.imread(mask_path_list[n],0)\n",
    "        mask = cv2.resize(mask, (128,128), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        one_hot = np.zeros((mask.shape[0], mask.shape[1], n_classes))\n",
    "        for i , unique_value in enumerate(np.unique(mask)):\n",
    "            one_hot[:, :, i][mask==unique_value]=1\n",
    "\n",
    "        X[n] = x_img / 255.0\n",
    "        if train:\n",
    "            y[n] = one_hot\n",
    "    print('Done!')\n",
    "    if train:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "X, y = get_data(train_img, train_mask, train=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segnet():\n",
    "    inputs = Input((128,128,3))\n",
    "    \n",
    "    encoder = Conv2D(32, (3, 3), padding='same')(inputs)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    encoder = Conv2D(32, (3, 3), padding='same')(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder0 = Activation('relu')(encoder)\n",
    "    endcoder0_pool = MaxPooling2D((2, 2), strides=(2, 2))(encoder0)\n",
    "    \n",
    "    encoder = Conv2D(64, (3, 3), padding='same')(endcoder0_pool)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    encoder = Conv2D(64, (3, 3), padding='same')(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder1 = Activation('relu')(encoder)\n",
    "    endcoder1_pool = MaxPooling2D((2, 2), strides=(2, 2))(encoder1)\n",
    "    \n",
    "    encoder = Conv2D(128, (3, 3), padding='same')(endcoder1_pool)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    encoder = Conv2D(128, (3, 3), padding='same')(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder2 = Activation('relu')(encoder)\n",
    "    endcoder2_pool = MaxPooling2D((2, 2), strides=(2, 2))(encoder2)\n",
    "    \n",
    "    encoder = Conv2D(256, (3, 3), padding='same')(endcoder2_pool)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    encoder = Conv2D(256, (3, 3), padding='same')(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder3 = Activation('relu')(encoder)\n",
    "    endcoder3_pool = MaxPooling2D((2, 2), strides=(2, 2))(encoder3)\n",
    "    \n",
    "    encoder = Conv2D(512, (3, 3), padding='same')(endcoder3_pool)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    encoder = Conv2D(512, (3, 3), padding='same')(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder4 = Activation('relu')(encoder)\n",
    "    endcoder4_pool = MaxPooling2D((2, 2), strides=(2, 2))(encoder4)\n",
    "    \n",
    "    center = Conv2D(1024, (3, 3), padding='same')(endcoder4_pool)\n",
    "    encoder = BatchNormalization()(center)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    encoder = Conv2D(1024, (3, 3), padding='same')(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    \n",
    "    decoder = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(encoder)\n",
    "    decoder = concatenate([encoder4, decoder], axis=-1)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(512, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(512, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    \n",
    "    decoder = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(decoder)\n",
    "    decoder = concatenate([encoder3, decoder], axis=-1)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(256, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(256, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    \n",
    "    decoder = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(decoder)\n",
    "    decoder = concatenate([encoder2, decoder], axis=-1)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(128, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(128, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    \n",
    "    decoder = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(decoder)\n",
    "    decoder = concatenate([encoder1, decoder], axis=-1)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(64, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(64, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    \n",
    "    decoder = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(decoder)\n",
    "    decoder = concatenate([encoder0, decoder], axis=-1)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(32, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(32, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    \n",
    "    outputs = Conv2D(21, (1,1), activation='sigmoid')(decoder)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = segnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 128)  147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 256)  590080      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 512)    2359808     activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 512)    2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 512)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 1024)   4719616     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 1024)   4096        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 1024)   9438208     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 1024)   4096        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 8, 8, 512)    2097664     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 1024)   0           activation_10[0][0]              \n",
      "                                                                 conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 1024)   4096        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 512)    4719104     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 512)    2359808     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 512)    2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 512)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 16, 16, 256)  524544      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 512)  0           activation_8[0][0]               \n",
      "                                                                 conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 512)  2048        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 512)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 256)  1179904     activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 256)  1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 256)  590080      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 256)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 128)  131200      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 256)  0           activation_6[0][0]               \n",
      "                                                                 conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 256)  1024        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 256)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 128)  295040      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 64, 64, 64)   32832       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 128)  0           activation_4[0][0]               \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   73792       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 64, 64, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 64, 64)   0           batch_normalization_23[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 64)   36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 64, 64, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 128, 128, 32) 8224        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, 128, 64) 0           activation_2[0][0]               \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 128, 128, 64) 256         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 128, 128, 64) 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 32) 18464       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 128, 128, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 128, 128, 32) 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 128, 128, 32) 9248        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 128, 128, 32) 128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 128, 128, 32) 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 128, 128, 21) 693         activation_27[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,127,157\n",
      "Trainable params: 31,111,157\n",
      "Non-trainable params: 16,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 32) 896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 128)  147584      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 128)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 256)  590080      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)    0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    1180160     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 512)    2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 512)    2359808     activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 512)    2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 512)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 512)    0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 1024)   4719616     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 1024)   4096        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 1024)   9438208     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 1024)   4096        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 8, 8, 512)    2097664     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 1024)   0           activation_10[0][0]              \n",
      "                                                                 conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 1024)   4096        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 512)    4719104     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 512)    2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 512)    2359808     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 512)    2048        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 512)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 16, 16, 256)  524544      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 512)  0           activation_8[0][0]               \n",
      "                                                                 conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 512)  2048        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 512)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 256)  1179904     activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 256)  1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 256)  590080      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 256)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 128)  131200      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 256)  0           activation_6[0][0]               \n",
      "                                                                 conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 256)  1024        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 256)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 128)  295040      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 64, 64, 64)   32832       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 128)  0           activation_4[0][0]               \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   73792       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 64, 64, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 64, 64)   0           batch_normalization_23[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 64)   36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 64, 64, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 128, 128, 32) 8224        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, 128, 64) 0           activation_2[0][0]               \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 128, 128, 64) 256         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 128, 128, 64) 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 32) 18464       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 128, 128, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 128, 128, 32) 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 128, 128, 32) 9248        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 128, 128, 32) 128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 128, 128, 32) 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 128, 128, 21) 693         activation_27[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,127,157\n",
      "Trainable params: 31,111,157\n",
      "Non-trainable params: 16,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss, 'accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 19 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - ETA: 55s - loss: 1.6623 - dice_loss: 0.9183 - accuracy: 0.009 - ETA: 24s - loss: 1.6503 - dice_loss: 0.9162 - accuracy: 0.010 - ETA: 4s - loss: 1.6387 - dice_loss: 0.9142 - accuracy: 0.011 - 60s 571ms/step - loss: 1.6352 - dice_loss: 0.9126 - accuracy: 0.0140 - val_loss: 1.5940 - val_dice_loss: 0.9126 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - ETA: 29s - loss: 1.5712 - dice_loss: 0.9029 - accuracy: 0.096 - ETA: 16s - loss: 1.5646 - dice_loss: 0.9021 - accuracy: 0.109 - ETA: 3s - loss: 1.5584 - dice_loss: 0.9017 - accuracy: 0.119 - 47s 447ms/step - loss: 1.5564 - dice_loss: 0.9011 - accuracy: 0.1250 - val_loss: 17.9878 - val_dice_loss: 0.9658 - val_accuracy: 0.0643\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - ETA: 28s - loss: 1.5136 - dice_loss: 0.8976 - accuracy: 0.212 - ETA: 15s - loss: 1.5078 - dice_loss: 0.8968 - accuracy: 0.241 - ETA: 3s - loss: 1.5029 - dice_loss: 0.8957 - accuracy: 0.255 - 42s 396ms/step - loss: 1.5017 - dice_loss: 0.8958 - accuracy: 0.2595 - val_loss: 822.6243 - val_dice_loss: 0.9770 - val_accuracy: 0.1164\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - ETA: 25s - loss: 1.4716 - dice_loss: 0.8916 - accuracy: 0.328 - ETA: 14s - loss: 1.4685 - dice_loss: 0.8915 - accuracy: 0.338 - ETA: 3s - loss: 1.4650 - dice_loss: 0.8911 - accuracy: 0.353 - 40s 381ms/step - loss: 1.4639 - dice_loss: 0.8908 - accuracy: 0.3552 - val_loss: 393.5115 - val_dice_loss: 0.9671 - val_accuracy: 0.1639\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - ETA: 25s - loss: 1.4401 - dice_loss: 0.8881 - accuracy: 0.420 - ETA: 14s - loss: 1.4399 - dice_loss: 0.8886 - accuracy: 0.423 - ETA: 3s - loss: 1.4351 - dice_loss: 0.8875 - accuracy: 0.438 - 41s 395ms/step - loss: 1.4343 - dice_loss: 0.8876 - accuracy: 0.4377 - val_loss: 193.2080 - val_dice_loss: 0.9695 - val_accuracy: 0.1473\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - ETA: 25s - loss: 1.4159 - dice_loss: 0.8856 - accuracy: 0.464 - ETA: 14s - loss: 1.4087 - dice_loss: 0.8833 - accuracy: 0.494 - ETA: 3s - loss: 1.4057 - dice_loss: 0.8827 - accuracy: 0.493 - 40s 386ms/step - loss: 1.4063 - dice_loss: 0.8839 - accuracy: 0.4868 - val_loss: 49.0593 - val_dice_loss: 0.9902 - val_accuracy: 0.0149\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 1.3869 - dice_loss: 0.8789 - accuracy: 0.535 - ETA: 14s - loss: 1.3825 - dice_loss: 0.8778 - accuracy: 0.542 - ETA: 3s - loss: 1.3807 - dice_loss: 0.8778 - accuracy: 0.532 - 41s 391ms/step - loss: 1.3814 - dice_loss: 0.8793 - accuracy: 0.5256 - val_loss: 38.1859 - val_dice_loss: 0.9825 - val_accuracy: 0.0369\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - ETA: 25s - loss: 1.3682 - dice_loss: 0.8762 - accuracy: 0.533 - ETA: 14s - loss: 1.3660 - dice_loss: 0.8757 - accuracy: 0.536 - ETA: 3s - loss: 1.3614 - dice_loss: 0.8743 - accuracy: 0.560 - 40s 380ms/step - loss: 1.3600 - dice_loss: 0.8734 - accuracy: 0.5586 - val_loss: 3.5340 - val_dice_loss: 0.9622 - val_accuracy: 0.0019\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - ETA: 25s - loss: 1.3497 - dice_loss: 0.8747 - accuracy: 0.510 - ETA: 14s - loss: 1.3412 - dice_loss: 0.8710 - accuracy: 0.561 - ETA: 3s - loss: 1.3353 - dice_loss: 0.8688 - accuracy: 0.586 - 40s 381ms/step - loss: 1.3358 - dice_loss: 0.8704 - accuracy: 0.5769 - val_loss: 2.9051 - val_dice_loss: 0.9579 - val_accuracy: 0.0080\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 25s - loss: 1.3177 - dice_loss: 0.8654 - accuracy: 0.613 - ETA: 14s - loss: 1.3132 - dice_loss: 0.8638 - accuracy: 0.611 - ETA: 3s - loss: 1.3126 - dice_loss: 0.8643 - accuracy: 0.598 - 40s 385ms/step - loss: 1.3116 - dice_loss: 0.8636 - accuracy: 0.6007 - val_loss: 4.4752 - val_dice_loss: 0.9546 - val_accuracy: 0.0182\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - ETA: 25s - loss: 1.2985 - dice_loss: 0.8622 - accuracy: 0.609 - ETA: 14s - loss: 1.2928 - dice_loss: 0.8600 - accuracy: 0.620 - ETA: 3s - loss: 1.2900 - dice_loss: 0.8595 - accuracy: 0.620 - 40s 384ms/step - loss: 1.2878 - dice_loss: 0.8569 - accuracy: 0.6283 - val_loss: 2.4883 - val_dice_loss: 0.9352 - val_accuracy: 0.0198\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 1.2734 - dice_loss: 0.8560 - accuracy: 0.637 - ETA: 14s - loss: 1.2708 - dice_loss: 0.8555 - accuracy: 0.644 - ETA: 3s - loss: 1.2645 - dice_loss: 0.8530 - accuracy: 0.663 - 41s 388ms/step - loss: 1.2637 - dice_loss: 0.8526 - accuracy: 0.6628 - val_loss: 2.8497 - val_dice_loss: 0.9476 - val_accuracy: 0.0254\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - ETA: 30s - loss: 1.2564 - dice_loss: 0.8538 - accuracy: 0.624 - ETA: 18s - loss: 1.2465 - dice_loss: 0.8493 - accuracy: 0.660 - ETA: 3s - loss: 1.2399 - dice_loss: 0.8466 - accuracy: 0.681 - 50s 472ms/step - loss: 1.2399 - dice_loss: 0.8473 - accuracy: 0.6764 - val_loss: 2.0271 - val_dice_loss: 0.9185 - val_accuracy: 0.0471\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - ETA: 32s - loss: 1.2197 - dice_loss: 0.8407 - accuracy: 0.683 - ETA: 17s - loss: 1.2187 - dice_loss: 0.8409 - accuracy: 0.682 - ETA: 3s - loss: 1.2179 - dice_loss: 0.8411 - accuracy: 0.678 - 51s 484ms/step - loss: 1.2164 - dice_loss: 0.8395 - accuracy: 0.6814 - val_loss: 1.4280 - val_dice_loss: 0.8753 - val_accuracy: 0.1035\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - ETA: 33s - loss: 1.1943 - dice_loss: 0.8332 - accuracy: 0.728 - ETA: 16s - loss: 1.1952 - dice_loss: 0.8346 - accuracy: 0.700 - ETA: 3s - loss: 1.1962 - dice_loss: 0.8360 - accuracy: 0.686 - 45s 430ms/step - loss: 1.1931 - dice_loss: 0.8315 - accuracy: 0.6964 - val_loss: 1.3289 - val_dice_loss: 0.8467 - val_accuracy: 0.3538\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 1.1745 - dice_loss: 0.8284 - accuracy: 0.720 - ETA: 14s - loss: 1.1722 - dice_loss: 0.8279 - accuracy: 0.704 - ETA: 3s - loss: 1.1697 - dice_loss: 0.8274 - accuracy: 0.703 - 41s 388ms/step - loss: 1.1686 - dice_loss: 0.8265 - accuracy: 0.7023 - val_loss: 1.2068 - val_dice_loss: 0.8374 - val_accuracy: 0.3961\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 1.1502 - dice_loss: 0.8207 - accuracy: 0.722 - ETA: 14s - loss: 1.1424 - dice_loss: 0.8171 - accuracy: 0.728 - ETA: 3s - loss: 1.1482 - dice_loss: 0.8211 - accuracy: 0.694 - 41s 392ms/step - loss: 1.1448 - dice_loss: 0.8158 - accuracy: 0.7046 - val_loss: 1.2497 - val_dice_loss: 0.8601 - val_accuracy: 0.5380\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 1.1412 - dice_loss: 0.8211 - accuracy: 0.660 - ETA: 14s - loss: 1.1235 - dice_loss: 0.8117 - accuracy: 0.708 - ETA: 3s - loss: 1.1191 - dice_loss: 0.8097 - accuracy: 0.713 - 41s 386ms/step - loss: 1.1202 - dice_loss: 0.8120 - accuracy: 0.7052 - val_loss: 1.1117 - val_dice_loss: 0.7748 - val_accuracy: 0.5439\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 1.1121 - dice_loss: 0.8092 - accuracy: 0.663 - ETA: 14s - loss: 1.1008 - dice_loss: 0.8032 - accuracy: 0.693 - ETA: 3s - loss: 1.1000 - dice_loss: 0.8033 - accuracy: 0.696 - 41s 391ms/step - loss: 1.0968 - dice_loss: 0.7983 - accuracy: 0.7073 - val_loss: 1.1828 - val_dice_loss: 0.8315 - val_accuracy: 0.3950\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - ETA: 30s - loss: 1.0729 - dice_loss: 0.7911 - accuracy: 0.732 - ETA: 18s - loss: 1.0759 - dice_loss: 0.7935 - accuracy: 0.709 - ETA: 3s - loss: 1.0723 - dice_loss: 0.7919 - accuracy: 0.709 - 49s 466ms/step - loss: 1.0721 - dice_loss: 0.7919 - accuracy: 0.7085 - val_loss: 1.2596 - val_dice_loss: 0.8590 - val_accuracy: 0.3774\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - ETA: 28s - loss: 1.0408 - dice_loss: 0.7762 - accuracy: 0.751 - ETA: 16s - loss: 1.0450 - dice_loss: 0.7790 - accuracy: 0.727 - ETA: 3s - loss: 1.0465 - dice_loss: 0.7804 - accuracy: 0.714 - 45s 427ms/step - loss: 1.0477 - dice_loss: 0.7829 - accuracy: 0.7083 - val_loss: 1.1231 - val_dice_loss: 0.8214 - val_accuracy: 0.5465\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 27s - loss: 1.0358 - dice_loss: 0.7772 - accuracy: 0.668 - ETA: 16s - loss: 1.0295 - dice_loss: 0.7732 - accuracy: 0.705 - ETA: 3s - loss: 1.0239 - dice_loss: 0.7703 - accuracy: 0.713 - 46s 433ms/step - loss: 1.0242 - dice_loss: 0.7713 - accuracy: 0.7082 - val_loss: 1.1422 - val_dice_loss: 0.8232 - val_accuracy: 0.6319\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - ETA: 27s - loss: 1.0018 - dice_loss: 0.7597 - accuracy: 0.708 - ETA: 15s - loss: 1.0078 - dice_loss: 0.7635 - accuracy: 0.703 - ETA: 3s - loss: 1.0039 - dice_loss: 0.7615 - accuracy: 0.701 - 45s 429ms/step - loss: 0.9995 - dice_loss: 0.7541 - accuracy: 0.7106 - val_loss: 1.1537 - val_dice_loss: 0.8291 - val_accuracy: 0.5562\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - ETA: 31s - loss: 0.9537 - dice_loss: 0.7334 - accuracy: 0.792 - ETA: 17s - loss: 0.9643 - dice_loss: 0.7400 - accuracy: 0.745 - ETA: 3s - loss: 0.9730 - dice_loss: 0.7455 - accuracy: 0.713 - 48s 455ms/step - loss: 0.9760 - dice_loss: 0.7509 - accuracy: 0.7098 - val_loss: 1.1203 - val_dice_loss: 0.8154 - val_accuracy: 0.6140\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - ETA: 29s - loss: 0.9593 - dice_loss: 0.7389 - accuracy: 0.702 - ETA: 16s - loss: 0.9500 - dice_loss: 0.7334 - accuracy: 0.725 - ETA: 3s - loss: 0.9561 - dice_loss: 0.7373 - accuracy: 0.703 - 47s 446ms/step - loss: 0.9529 - dice_loss: 0.7318 - accuracy: 0.7092 - val_loss: 1.1702 - val_dice_loss: 0.8283 - val_accuracy: 0.6298\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - ETA: 31s - loss: 0.9248 - dice_loss: 0.7195 - accuracy: 0.716 - ETA: 26s - loss: 0.9265 - dice_loss: 0.7206 - accuracy: 0.716 - ETA: 7s - loss: 0.9279 - dice_loss: 0.7218 - accuracy: 0.708 - 86s 818ms/step - loss: 0.9284 - dice_loss: 0.7227 - accuracy: 0.7089 - val_loss: 1.1115 - val_dice_loss: 0.8092 - val_accuracy: 0.6862\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - ETA: 47s - loss: 0.9371 - dice_loss: 0.7290 - accuracy: 0.648 - ETA: 31s - loss: 0.9148 - dice_loss: 0.7150 - accuracy: 0.692 - ETA: 8s - loss: 0.9068 - dice_loss: 0.7100 - accuracy: 0.708 - 108s 1s/step - loss: 0.9055 - dice_loss: 0.7077 - accuracy: 0.7088 - val_loss: 1.0960 - val_dice_loss: 0.7980 - val_accuracy: 0.7026\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 1:21 - loss: 0.9042 - dice_loss: 0.7093 - accuracy: 0.66 - ETA: 45s - loss: 0.8953 - dice_loss: 0.7036 - accuracy: 0.6801 - ETA: 10s - loss: 0.8846 - dice_loss: 0.6969 - accuracy: 0.702 - 129s 1s/step - loss: 0.8811 - dice_loss: 0.6906 - accuracy: 0.7083 - val_loss: 1.0676 - val_dice_loss: 0.7873 - val_accuracy: 0.7031\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - ETA: 1:00 - loss: 0.8767 - dice_loss: 0.6922 - accuracy: 0.71 - ETA: 41s - loss: 0.8651 - dice_loss: 0.6849 - accuracy: 0.7232 - ETA: 8s - loss: 0.8580 - dice_loss: 0.6807 - accuracy: 0.719 - 106s 1s/step - loss: 0.8610 - dice_loss: 0.6865 - accuracy: 0.7077 - val_loss: 0.9583 - val_dice_loss: 0.7340 - val_accuracy: 0.7117\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 51s - loss: 0.8622 - dice_loss: 0.6839 - accuracy: 0.670 - ETA: 23s - loss: 0.8479 - dice_loss: 0.6747 - accuracy: 0.700 - ETA: 4s - loss: 0.8427 - dice_loss: 0.6712 - accuracy: 0.703 - 55s 527ms/step - loss: 0.8384 - dice_loss: 0.6630 - accuracy: 0.7111 - val_loss: 1.0213 - val_dice_loss: 0.7655 - val_accuracy: 0.6924\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 0.8030 - dice_loss: 0.6451 - accuracy: 0.765 - ETA: 14s - loss: 0.8185 - dice_loss: 0.6554 - accuracy: 0.720 - ETA: 3s - loss: 0.8220 - dice_loss: 0.6577 - accuracy: 0.708 - 42s 399ms/step - loss: 0.8184 - dice_loss: 0.6508 - accuracy: 0.7135 - val_loss: 0.9416 - val_dice_loss: 0.7247 - val_accuracy: 0.6973\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 0.8173 - dice_loss: 0.6546 - accuracy: 0.687 - ETA: 14s - loss: 0.7879 - dice_loss: 0.6354 - accuracy: 0.730 - ETA: 3s - loss: 0.7990 - dice_loss: 0.6427 - accuracy: 0.705 - 41s 395ms/step - loss: 0.7953 - dice_loss: 0.6357 - accuracy: 0.7086 - val_loss: 0.9367 - val_dice_loss: 0.7222 - val_accuracy: 0.7135\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 0.7589 - dice_loss: 0.6158 - accuracy: 0.751 - ETA: 14s - loss: 0.7819 - dice_loss: 0.6310 - accuracy: 0.701 - ETA: 3s - loss: 0.7688 - dice_loss: 0.6222 - accuracy: 0.718 - 42s 396ms/step - loss: 0.7724 - dice_loss: 0.6291 - accuracy: 0.7137 - val_loss: 0.8931 - val_dice_loss: 0.6979 - val_accuracy: 0.7163\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - ETA: 25s - loss: 0.7747 - dice_loss: 0.6260 - accuracy: 0.650 - ETA: 14s - loss: 0.7558 - dice_loss: 0.6132 - accuracy: 0.696 - ETA: 3s - loss: 0.7516 - dice_loss: 0.6100 - accuracy: 0.706 - 41s 394ms/step - loss: 0.7497 - dice_loss: 0.6063 - accuracy: 0.7119 - val_loss: 0.9096 - val_dice_loss: 0.7062 - val_accuracy: 0.7298\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 0.7367 - dice_loss: 0.5997 - accuracy: 0.693 - ETA: 14s - loss: 0.7409 - dice_loss: 0.6021 - accuracy: 0.702 - ETA: 3s - loss: 0.7362 - dice_loss: 0.5991 - accuracy: 0.699 - 41s 390ms/step - loss: 0.7286 - dice_loss: 0.5842 - accuracy: 0.7104 - val_loss: 0.9337 - val_dice_loss: 0.7213 - val_accuracy: 0.5658\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 0.7376 - dice_loss: 0.5991 - accuracy: 0.672 - ETA: 14s - loss: 0.7114 - dice_loss: 0.5814 - accuracy: 0.714 - ETA: 3s - loss: 0.7170 - dice_loss: 0.5850 - accuracy: 0.705 - 47s 443ms/step - loss: 0.7096 - dice_loss: 0.5702 - accuracy: 0.7156 - val_loss: 0.8924 - val_dice_loss: 0.7068 - val_accuracy: 0.5645\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 37s - loss: 0.7093 - dice_loss: 0.5785 - accuracy: 0.698 - ETA: 19s - loss: 0.6933 - dice_loss: 0.5677 - accuracy: 0.716 - ETA: 4s - loss: 0.6889 - dice_loss: 0.5648 - accuracy: 0.718 - 50s 473ms/step - loss: 0.6911 - dice_loss: 0.5686 - accuracy: 0.7143 - val_loss: 0.8255 - val_dice_loss: 0.6600 - val_accuracy: 0.7212\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 0.6743 - dice_loss: 0.5526 - accuracy: 0.726 - ETA: 15s - loss: 0.6987 - dice_loss: 0.5691 - accuracy: 0.683 - ETA: 3s - loss: 0.6820 - dice_loss: 0.5575 - accuracy: 0.704 - 43s 408ms/step - loss: 0.6774 - dice_loss: 0.5482 - accuracy: 0.7098 - val_loss: 0.8160 - val_dice_loss: 0.6505 - val_accuracy: 0.6452\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 0.7024 - dice_loss: 0.5699 - accuracy: 0.660 - ETA: 14s - loss: 0.6653 - dice_loss: 0.5448 - accuracy: 0.701 - ETA: 3s - loss: 0.6602 - dice_loss: 0.5411 - accuracy: 0.708 - 41s 392ms/step - loss: 0.6602 - dice_loss: 0.5408 - accuracy: 0.7093 - val_loss: 0.8010 - val_dice_loss: 0.6408 - val_accuracy: 0.6089\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 26s - loss: 0.6469 - dice_loss: 0.5301 - accuracy: 0.706 - ETA: 14s - loss: 0.6457 - dice_loss: 0.5296 - accuracy: 0.707 - ETA: 3s - loss: 0.6376 - dice_loss: 0.5240 - accuracy: 0.716 - 41s 392ms/step - loss: 0.6411 - dice_loss: 0.5308 - accuracy: 0.7128 - val_loss: 0.7555 - val_dice_loss: 0.6085 - val_accuracy: 0.6146\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - ETA: 27s - loss: 0.6080 - dice_loss: 0.5026 - accuracy: 0.730 - ETA: 15s - loss: 0.6043 - dice_loss: 0.4999 - accuracy: 0.729 - ETA: 3s - loss: 0.6255 - dice_loss: 0.5140 - accuracy: 0.707 - 44s 418ms/step - loss: 0.6247 - dice_loss: 0.5121 - accuracy: 0.7088 - val_loss: 0.7466 - val_dice_loss: 0.5792 - val_accuracy: 0.5201\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - ETA: 29s - loss: 0.6229 - dice_loss: 0.5106 - accuracy: 0.693 - ETA: 15s - loss: 0.6227 - dice_loss: 0.5106 - accuracy: 0.693 - ETA: 3s - loss: 0.6082 - dice_loss: 0.5003 - accuracy: 0.715 - 43s 411ms/step - loss: 0.6073 - dice_loss: 0.4983 - accuracy: 0.7171 - val_loss: 0.7806 - val_dice_loss: 0.5558 - val_accuracy: 0.5109\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 26s - loss: 0.5823 - dice_loss: 0.4807 - accuracy: 0.716 - ETA: 14s - loss: 0.5834 - dice_loss: 0.4811 - accuracy: 0.727 - ETA: 3s - loss: 0.5916 - dice_loss: 0.4864 - accuracy: 0.716 - 41s 393ms/step - loss: 0.5915 - dice_loss: 0.4860 - accuracy: 0.7187 - val_loss: 0.7403 - val_dice_loss: 0.5614 - val_accuracy: 0.5621\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - ETA: 28s - loss: 0.5717 - dice_loss: 0.4720 - accuracy: 0.723 - ETA: 16s - loss: 0.5797 - dice_loss: 0.4770 - accuracy: 0.711 - ETA: 3s - loss: 0.5672 - dice_loss: 0.4681 - accuracy: 0.724 - 47s 450ms/step - loss: 0.5761 - dice_loss: 0.4856 - accuracy: 0.7163 - val_loss: 0.7245 - val_dice_loss: 0.5701 - val_accuracy: 0.5760\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - ETA: 29s - loss: 0.5345 - dice_loss: 0.4442 - accuracy: 0.764 - ETA: 16s - loss: 0.5573 - dice_loss: 0.4598 - accuracy: 0.735 - ETA: 3s - loss: 0.5661 - dice_loss: 0.4654 - accuracy: 0.721 - 48s 460ms/step - loss: 0.5657 - dice_loss: 0.4646 - accuracy: 0.7217 - val_loss: 0.7170 - val_dice_loss: 0.5818 - val_accuracy: 0.5754\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 34s - loss: 0.5612 - dice_loss: 0.4611 - accuracy: 0.707 - ETA: 18s - loss: 0.5489 - dice_loss: 0.4523 - accuracy: 0.729 - ETA: 4s - loss: 0.5486 - dice_loss: 0.4516 - accuracy: 0.731 - 52s 492ms/step - loss: 0.5478 - dice_loss: 0.4499 - accuracy: 0.7326 - val_loss: 0.7649 - val_dice_loss: 0.6208 - val_accuracy: 0.5557\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - ETA: 31s - loss: 0.5215 - dice_loss: 0.4311 - accuracy: 0.761 - ETA: 18s - loss: 0.5683 - dice_loss: 0.4632 - accuracy: 0.699 - ETA: 3s - loss: 0.5495 - dice_loss: 0.4501 - accuracy: 0.719 - 51s 485ms/step - loss: 0.5412 - dice_loss: 0.4330 - accuracy: 0.7272 - val_loss: 0.7408 - val_dice_loss: 0.5978 - val_accuracy: 0.4830\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - ETA: 31s - loss: 0.5506 - dice_loss: 0.4501 - accuracy: 0.701 - ETA: 19s - loss: 0.5399 - dice_loss: 0.4422 - accuracy: 0.717 - ETA: 4s - loss: 0.5258 - dice_loss: 0.4319 - accuracy: 0.735 - 53s 508ms/step - loss: 0.5229 - dice_loss: 0.4258 - accuracy: 0.7389 - val_loss: 0.7142 - val_dice_loss: 0.5686 - val_accuracy: 0.5224\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - ETA: 30s - loss: 0.5348 - dice_loss: 0.4369 - accuracy: 0.721 - ETA: 16s - loss: 0.5020 - dice_loss: 0.4134 - accuracy: 0.760 - ETA: 3s - loss: 0.5131 - dice_loss: 0.4208 - accuracy: 0.746 - 47s 444ms/step - loss: 0.5128 - dice_loss: 0.4197 - accuracy: 0.7469 - val_loss: 0.7301 - val_dice_loss: 0.5710 - val_accuracy: 0.5035\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - ETA: 29s - loss: 0.5040 - dice_loss: 0.4132 - accuracy: 0.749 - ETA: 16s - loss: 0.5166 - dice_loss: 0.4219 - accuracy: 0.733 - ETA: 3s - loss: 0.5055 - dice_loss: 0.4138 - accuracy: 0.743 - 48s 455ms/step - loss: 0.5045 - dice_loss: 0.4116 - accuracy: 0.7431 - val_loss: 0.7786 - val_dice_loss: 0.6108 - val_accuracy: 0.4133\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 32s - loss: 0.4578 - dice_loss: 0.3794 - accuracy: 0.783 - ETA: 19s - loss: 0.4575 - dice_loss: 0.3786 - accuracy: 0.783 - ETA: 4s - loss: 0.4975 - dice_loss: 0.4065 - accuracy: 0.740 - 59s 560ms/step - loss: 0.4981 - dice_loss: 0.4076 - accuracy: 0.7392 - val_loss: 0.7896 - val_dice_loss: 0.6214 - val_accuracy: 0.4204\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 39s - loss: 0.4987 - dice_loss: 0.4057 - accuracy: 0.731 - ETA: 21s - loss: 0.4905 - dice_loss: 0.3997 - accuracy: 0.742 - ETA: 4s - loss: 0.4827 - dice_loss: 0.3944 - accuracy: 0.750 - 57s 547ms/step - loss: 0.4866 - dice_loss: 0.4022 - accuracy: 0.7466 - val_loss: 0.8379 - val_dice_loss: 0.6274 - val_accuracy: 0.3628\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - ETA: 33s - loss: 0.4480 - dice_loss: 0.3690 - accuracy: 0.777 - ETA: 19s - loss: 0.4809 - dice_loss: 0.3918 - accuracy: 0.748 - ETA: 4s - loss: 0.4884 - dice_loss: 0.3968 - accuracy: 0.740 - 55s 523ms/step - loss: 0.4805 - dice_loss: 0.3806 - accuracy: 0.7466 - val_loss: 0.8258 - val_dice_loss: 0.6185 - val_accuracy: 0.3544\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - ETA: 34s - loss: 0.4682 - dice_loss: 0.3806 - accuracy: 0.756 - ETA: 19s - loss: 0.4663 - dice_loss: 0.3790 - accuracy: 0.754 - ETA: 4s - loss: 0.4711 - dice_loss: 0.3823 - accuracy: 0.749 - 54s 510ms/step - loss: 0.4643 - dice_loss: 0.3685 - accuracy: 0.7547 - val_loss: 0.7871 - val_dice_loss: 0.6117 - val_accuracy: 0.3858\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - ETA: 38s - loss: 0.4596 - dice_loss: 0.3748 - accuracy: 0.748 - ETA: 21s - loss: 0.4462 - dice_loss: 0.3649 - accuracy: 0.762 - ETA: 4s - loss: 0.4574 - dice_loss: 0.3725 - accuracy: 0.753 - 54s 518ms/step - loss: 0.4583 - dice_loss: 0.3743 - accuracy: 0.7521 - val_loss: 0.8727 - val_dice_loss: 0.6453 - val_accuracy: 0.3536\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - ETA: 33s - loss: 0.4533 - dice_loss: 0.3677 - accuracy: 0.757 - ETA: 18s - loss: 0.4556 - dice_loss: 0.3697 - accuracy: 0.751 - ETA: 4s - loss: 0.4482 - dice_loss: 0.3643 - accuracy: 0.757 - 52s 497ms/step - loss: 0.4470 - dice_loss: 0.3615 - accuracy: 0.7583 - val_loss: 0.7727 - val_dice_loss: 0.5926 - val_accuracy: 0.4363\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - ETA: 33s - loss: 0.4290 - dice_loss: 0.3491 - accuracy: 0.774 - ETA: 18s - loss: 0.4149 - dice_loss: 0.3394 - accuracy: 0.785 - ETA: 4s - loss: 0.4306 - dice_loss: 0.3502 - accuracy: 0.769 - 57s 543ms/step - loss: 0.4370 - dice_loss: 0.3635 - accuracy: 0.7630 - val_loss: 0.7662 - val_dice_loss: 0.5775 - val_accuracy: 0.4659\n",
      "Epoch 58/100\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=100, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(onehot_to_rgb(model.predict(X_train[1].reshape(1,128,128,3)).reshape(128,128,21)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters):model.fit_generator(train_gen, steps_per_epoch = 100, epochs = 10)\n",
    "\n",
    "    encoder = Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    encoder = Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Activation('relu')(encoder)\n",
    "    return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    encoder = conv_block(input_tensor, num_filters)\n",
    "    encoder_pool = MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "\n",
    "    return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "    decoder = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "    decoder = concatenate([concat_tensor, decoder], axis=-1)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    decoder = Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = Activation('relu')(decoder)\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((128,128,3))\n",
    "\n",
    "encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
    "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
    "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
    "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
    "encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
    "\n",
    "center = conv_block(encoder4_pool, 1024)\n",
    "\n",
    "decoder4 = decoder_block(center, encoder4, 512)\n",
    "decoder3 = decoder_block(decoder4, encoder3, 256)\n",
    "decoder2 = decoder_block(decoder3, encoder2, 128)\n",
    "decoder1 = decoder_block(decoder2, encoder1, 64)\n",
    "decoder0 = decoder_block(decoder1, encoder0, 32)\n",
    "\n",
    "outputs = Conv2D(21, (1, 1), activation='sigmoid')(decoder0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer = 'adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b6c84792cc2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch = 100, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
