{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[!img]segnet.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(64,64, kernel_size=3, padding=1)\n",
    "                \n",
    "        self.conv3 = nn.Conv2d(64,128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128,128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128,256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(256,256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(256,512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(512,512, kernel_size=3, padding=1)        \n",
    "        \n",
    "        self.upSample = nn.MaxUnpool2d(2, stride=2)\n",
    "      \n",
    "        self.decode1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.decode2 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec_bn1 = nn.BatchNorm2d(512)\n",
    "        self.dec_relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.dec_bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.decode3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.decode4 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec_bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.decode5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.decode6 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dec_bn4 = nn.BatchNorm2d(64)\n",
    "        self.decode7 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.decode8 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out= self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        x1p, id1 = F.max_pool2d(out,kernel_size=2, stride=2,return_indices=True)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxPool(out)\n",
    "        x2p, id2 = F.max_pool2d(out,kernel_size=2, stride=2,return_indices=True)\n",
    "                \n",
    "        out = self.conv5(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        x3p, id3 = F.max_pool2d(out,kernel_size=2, stride=2,return_indices=True)\n",
    "        \n",
    "        out = self.conv7(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv8(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv8(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.relu(out)\n",
    "        x4p, id4 = F.max_pool2d(out,kernel_size=2, stride=2,return_indices=True)\n",
    "        \n",
    "        out = self.conv8(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv8(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv8(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.relu(out)\n",
    "        x5p, id5 = F.max_pool2d(out,kernel_size=2, stride=2,return_indices=True)\n",
    "        \n",
    "        out = self.upSample(x5p, id5)\n",
    "        out = self.decode1(out)\n",
    "        out = self.dec_bn1(out)\n",
    "        out = self.dec_relu(out)\n",
    "        out = self.decode1(out)\n",
    "        out = self.dec_bn1(out)\n",
    "        out = self.dec_relu(out)\n",
    "        out = self.decode1(out)\n",
    "        out = self.dec_bn1(out)\n",
    "        out = self.dec_relu(out)\n",
    "        \n",
    "        out = self.upSample(out, id4)\n",
    "        out = self.decode1(out)\n",
    "        out = self.dec_bn1(out)\n",
    "        out = self.dec_relu(out)\n",
    "        out = self.decode1(out)\n",
    "        out = self.dec_bn1(out)\n",
    "        out = self.dec_relu(out)\n",
    "        out = self.decode2(out)\n",
    "        out = self.dec_bn2(out)\n",
    "        out = self.dec_relu(out)\n",
    "        \n",
    "        out = self.upSample(out, id3)\n",
    "        out = self.decode3(out)\n",
    "        out = self.dec_bn2(out)\n",
    "        out = self.dec_relu(out)\n",
    "        out = self.decode3(out)\n",
    "        out = self.dec_bn2(out)\n",
    "        out = self.dec_relu(out)\n",
    "        out = self.decode4(out)\n",
    "        out = self.dec_bn3(out)\n",
    "        out = self.dec_relu(out)\n",
    "        \n",
    "        out = self.upSample(out, id2)\n",
    "        out = self.decode5(out)\n",
    "        out = self.dec_bn3(out)\n",
    "        out = self.dec_relu(out)\n",
    "        out = self.decode6(out)\n",
    "        out = self.dec_bn4(out)\n",
    "        out = self.dec_relu(out)\n",
    "        \n",
    "        out = self.upSample(out, id1)\n",
    "        out = self.decode7(out)\n",
    "        out = self.dec_bn4(out)\n",
    "        out = self.dec_relu(out)\n",
    "        out = self.decode8(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upSample): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "  (decode1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decode2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dec_bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dec_relu): ReLU(inplace=True)\n",
       "  (dec_bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (decode3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decode4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dec_bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (decode5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decode6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (dec_bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (decode7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (decode8): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation w/ VOC2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img=[]\n",
    "train_mask=[]\n",
    "trainval_img=[]\n",
    "trainval_mask=[]\n",
    "val_img=[]\n",
    "val_mask=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset  # For custom data-sets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "PATH = \"../VOC_2011_TrainVal/VOCdevkit/VOC2011/\"\n",
    "#train = open(PATH +\"../Data/ImageSets/Segmentation/train.txt\", 'r')\n",
    "train = open(PATH +\"ImageSets/Segmentation/train.txt\", 'r')\n",
    "lines = train.readlines()\n",
    "for i, line in enumerate(lines):\n",
    "    train_img.append(PATH +\"JPEGImages/\"+line.split('\\n')[0]+\".jpg\")\n",
    "    train_mask.append(PATH + \"SegmentationClass/\" + line.split('\\n')[0] + \".png\")\n",
    "train.close()\n",
    "\n",
    "trainval = open(PATH +\"ImageSets/Segmentation/trainval.txt\", 'r')\n",
    "lines = trainval.readlines()\n",
    "for i, line in enumerate(lines):\n",
    "    train_img.append(PATH +\"JPEGImages/\"+line.split('\\n')[0]+\".jpg\")\n",
    "    train_mask.append(PATH + \"SegmentationClass/\"+line.split('\\n')[0]+\".png\")\n",
    "trainval.close()\n",
    "\n",
    "val = open(PATH +\"ImageSets/Segmentation/val.txt\", 'r')\n",
    "lines = val.readlines()\n",
    "for i, line in enumerate(lines):\n",
    "    val_img.append(PATH +\"JPEGImages/\"+line.split('\\n')[0]+\".jpg\")\n",
    "    val_mask.append(PATH +\"SegmentationClass/\"+line.split('\\n')[0]+\".png\")\n",
    "val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, target_paths, train=True):   # initial logic happens like transform\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.transforms = transforms.ToTensor()\n",
    "        \n",
    "    def transform(self, image, mask):\n",
    "        resize=transforms.Resize(size=(520,520))\n",
    "        image=resize(image)\n",
    "        mask=resize(mask)\n",
    "        \n",
    "        # Random crop\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(\n",
    "            image, output_size=(512, 512))\n",
    "        image = TF.crop(image, i, j, h, w)\n",
    "        mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        num=random.random()\n",
    "        if num > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if num > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "\n",
    "         # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "\n",
    "        mean=[0.4546, 0.4405, 0.4051]\n",
    "        std=[0.2322, 0.2282, 0.2337]\n",
    "        image[0]=(image[0]-mean[0])/std[0]\n",
    "        image[1]=(image[1]-mean[1])/std[1]\n",
    "        image[2]=(image[2]-mean[2])/std[2]\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = Image.open(self.image_paths[index]).convert(\"RGB\")\n",
    "        mask = Image.open(self.target_paths[index]).convert(\"P\")\n",
    "\n",
    "        image, label=self.transform(image, mask)\n",
    "        \n",
    "        label_size=label.size\n",
    "        \n",
    "        # One-hot encoding \n",
    "        h = label.shape[1]\n",
    "        w= label.shape[2]\n",
    "        target = torch.zeros(21, h, w)\n",
    "        \n",
    "        label.reshape((-1,2))\n",
    "        for c in range(21):\n",
    "            target[c][label[0] == c] = 1\n",
    "\n",
    "        return {'x': image, 'y':target, 'l':label[0]} \n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):  # return count of sample we have\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_img, train_mask, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "trainval_dataset = CustomDataset(trainval_img, trainval_mask, train=False)\n",
    "trainval_loader = torch.utils.data.DataLoader(trainval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "val_dataset = CustomDataset(val_img, val_mask, train=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([8, 3, 512, 512]) torch.Size([8, 21, 512, 512]) torch.Size([8, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          \n",
    "    \n",
    "import random\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(i, batch['x'].shape, batch['y'].shape, batch['l'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 2.00 GiB total capacity; 1.26 GiB already allocated; 11.00 MiB free; 17.19 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6eaac0488416>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 2.00 GiB total capacity; 1.26 GiB already allocated; 11.00 MiB free; 17.19 MiB cached)"
     ]
    }
   ],
   "source": [
    "best = 0\n",
    "meaniou=100\n",
    "epochs=100\n",
    "model = model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch['x'].to(device), batch['y'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % print_every == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                #paccuracy, meaniou = validation(model, validloader, criterion)\n",
    "                print(\"Epoch: {}/{}.. \".format(epoch+1, epochs),\n",
    "                      \"Training loss: {:.5f}.. \".format(loss.item()))\n",
    "            if best < meaniou:\n",
    "                torch.save(model.state_dict(), \"./segnet.pth\")\n",
    "                best = meaniou\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
