{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 25\n",
    "\n",
    "input_texts = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 12\n",
      "Found 3056 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "for line in open('robert_frost.txt', encoding='UTF-8'):\n",
    "    line = line.rstrip()\n",
    "    \n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    input_line = '<sos> ' + line\n",
    "    \n",
    "    target_line = line + ' <eos>'\n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)\n",
    "\n",
    "all_lines = input_texts + target_texts\n",
    "\n",
    "# convert the sentences (strings) into integers\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer.fit_on_texts(all_lines)\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "# find max seq length\n",
    "max_sequence_length_from_data = max(len(s) for s in input_sequences)\n",
    "print('Max sequence length:', max_sequence_length_from_data)\n",
    "\n",
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))\n",
    "assert('<sos>' in word2idx)\n",
    "assert('<eos>' in word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glove.6B.50d.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('glove.6B.%sd.txt' % EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1436, 12)\n",
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "max_sequence_length = min(max_sequence_length_from_data, MAX_SEQUENCE_LENGTH)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "print('Shape of data tensor:', input_sequences.shape)\n",
    "\n",
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open('../glove.6B/'+os.path.join('glove.6B.%sd.txt' % EMBEDDING_DIM), encoding='UTF-8') as f:\n",
    "    # is just a space-separated text file in the format:\n",
    "    # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word2idx.items():\n",
    "    if i < MAX_VOCAB_SIZE:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot the targets (can't use sparse cross-entropy)\n",
    "\n",
    "one_hot_targets = np.zeros((len(input_sequences), max_sequence_length, num_words))\n",
    "\n",
    "for i, target_sequence in enumerate(target_sequences):\n",
    "    for t, word in enumerate(target_sequence):\n",
    "        if word > 0:\n",
    "            one_hot_targets[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  # trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Training model...\n",
      "Train on 1148 samples, validate on 288 samples\n",
      "Epoch 1/100\n",
      "1148/1148 [==============================] - 13s 11ms/step - loss: 5.3882 - accuracy: 0.0444 - val_loss: 5.0066 - val_accuracy: 0.0226\n",
      "Epoch 2/100\n",
      "1148/1148 [==============================] - 4s 3ms/step - loss: 4.5284 - accuracy: 0.0335 - val_loss: 4.7184 - val_accuracy: 0.0229\n",
      "Epoch 3/100\n",
      "1148/1148 [==============================] - 4s 4ms/step - loss: 4.1822 - accuracy: 0.0722 - val_loss: 4.7148 - val_accuracy: 0.0813\n",
      "Epoch 4/100\n",
      "1148/1148 [==============================] - 5s 4ms/step - loss: 4.0570 - accuracy: 0.0833 - val_loss: 4.6921 - val_accuracy: 0.0813\n",
      "Epoch 5/100\n",
      "1148/1148 [==============================] - 4s 4ms/step - loss: 3.9534 - accuracy: 0.0834 - val_loss: 4.6868 - val_accuracy: 0.0819\n",
      "Epoch 6/100\n",
      "1148/1148 [==============================] - 4s 3ms/step - loss: 3.8755 - accuracy: 0.0944 - val_loss: 4.6931 - val_accuracy: 0.0885\n",
      "Epoch 7/100\n",
      "1148/1148 [==============================] - 3s 3ms/step - loss: 3.8038 - accuracy: 0.1004 - val_loss: 4.6784 - val_accuracy: 0.0868\n",
      "Epoch 8/100\n",
      "1148/1148 [==============================] - 4s 3ms/step - loss: 3.7410 - accuracy: 0.1003 - val_loss: 4.6815 - val_accuracy: 0.0894\n",
      "Epoch 9/100\n",
      "1148/1148 [==============================] - 3s 3ms/step - loss: 3.6788 - accuracy: 0.1037 - val_loss: 4.6884 - val_accuracy: 0.0883\n",
      "Epoch 10/100\n",
      "1148/1148 [==============================] - 4s 3ms/step - loss: 3.6172 - accuracy: 0.1054 - val_loss: 4.6775 - val_accuracy: 0.0888\n",
      "Epoch 11/100\n",
      "1148/1148 [==============================] - 3s 3ms/step - loss: 3.5572 - accuracy: 0.1066 - val_loss: 4.6883 - val_accuracy: 0.0891\n",
      "Epoch 12/100\n",
      "1148/1148 [==============================] - 5s 4ms/step - loss: 3.4974 - accuracy: 0.1108 - val_loss: 4.6888 - val_accuracy: 0.0894\n",
      "Epoch 13/100\n",
      "1148/1148 [==============================] - 6s 5ms/step - loss: 3.4393 - accuracy: 0.1154 - val_loss: 4.6849 - val_accuracy: 0.0932\n",
      "Epoch 14/100\n",
      "1148/1148 [==============================] - 3s 3ms/step - loss: 3.3828 - accuracy: 0.1309 - val_loss: 4.6915 - val_accuracy: 0.0958\n",
      "Epoch 15/100\n",
      "1148/1148 [==============================] - 3s 3ms/step - loss: 3.3276 - accuracy: 0.1349 - val_loss: 4.6928 - val_accuracy: 0.0975\n",
      "Epoch 16/100\n",
      "1148/1148 [==============================] - 4s 4ms/step - loss: 3.2741 - accuracy: 0.1437 - val_loss: 4.6963 - val_accuracy: 0.1001\n",
      "Epoch 17/100\n",
      " 896/1148 [======================>.......] - ETA: 0s - loss: 3.2182 - accuracy: 0.1495"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "\n",
    "# create an LSTM network with a single LSTM\n",
    "input_ = Input(shape=(max_sequence_length,))\n",
    "initial_h = Input(shape=(LATENT_DIM,))\n",
    "initial_c = Input(shape=(LATENT_DIM,))\n",
    "\n",
    "x = embedding_layer(input_)\n",
    "lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "x, _, _ = lstm(x, initial_state=[initial_h, initial_c]) # don't need the states here\n",
    "dense = Dense(num_words, activation='softmax')\n",
    "output = dense(x)\n",
    "\n",
    "model = Model([input_, initial_h, initial_c], output)\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  # optimizer='rmsprop',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  # optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('Training model...')\n",
    "z = np.zeros((len(input_sequences), LATENT_DIM))\n",
    "r = model.fit(\n",
    "  [input_sequences, z, z],\n",
    "  one_hot_targets,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "\n",
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# make a sampling model\n",
    "input2 = Input(shape=(1,)) # we'll only input one word at a time\n",
    "x = embedding_layer(input2)\n",
    "x, h, c = lstm(x, initial_state=[initial_h, initial_c]) # now we need states to feed back in\n",
    "output2 = dense(x)\n",
    "sampling_model = Model([input2, initial_h, initial_c], [output2, h, c])\n",
    "\n",
    "\n",
    "# reverse word2idx dictionary to get back words\n",
    "# during prediction\n",
    "idx2word = {v:k for k, v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_line():\n",
    "    # initial inputs\n",
    "    np_input = np.array([[ word2idx['<sos>'] ]])\n",
    "    h = np.zeros((1, LATENT_DIM))\n",
    "    c = np.zeros((1, LATENT_DIM))\n",
    "\n",
    "    # so we know when to quit\n",
    "    eos = word2idx['<eos>']\n",
    "\n",
    "    # store the output here\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_sequence_length):\n",
    "        o, h, c = sampling_model.predict([np_input, h, c])\n",
    "\n",
    "        # print(\"o.shape:\", o.shape, o[0,0,:10])\n",
    "        # idx = np.argmax(o[0,0])\n",
    "        probs = o[0,0]\n",
    "        if np.argmax(probs) == 0:\n",
    "            print(\"wtf\")\n",
    "        probs[0] = 0\n",
    "        probs /= probs.sum()\n",
    "        idx = np.random.choice(len(probs), p=probs)\n",
    "        if idx == eos:\n",
    "            break\n",
    "\n",
    "        # accuulate output\n",
    "        output_sentence.append(idx2word.get(idx, '<WTF %s>' % idx))\n",
    "\n",
    "        # make the next input into model\n",
    "        np_input[0,0] = idx\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 4 line poem\n",
    "\n",
    "while True:\n",
    "    for _ in range(4):\n",
    "        print(sample_line())\n",
    "\n",
    "    ans = input(\"---generate another? [Y/n]---\")\n",
    "    if ans and ans[0].lower().startswith('n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
